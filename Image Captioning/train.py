import os
from tqdm import tqdm
import keras.callbacks as keras_callbacks

from model import get_head_model
import data_loading
import config as C


def train_with_full_ds_loaded(n_epochs=20):
    trs_X_img, trs_X_txt, trs_y, vocab_size, max_length = data_loading.load_ds(C.trs_names_file_path)
    val_X_img, val_X_txt, val_y, _, _ = data_loading.load_ds(C.val_names_file_path)

    # Create model
    model = get_head_model(vocab_size, max_length)

    # Define checkpoint callback
    filepath = os.path.join(C.MODELS_FOLDER, 'captioning_model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5')
    checkpoint = keras_callbacks.ModelCheckpoint(
        filepath, 
        monitor='val_loss', 
        verbose=1, 
        save_best_only=True, 
        mode='min'
    )

    # Train
    model.fit(
        [trs_X_img, trs_X_txt], 
        trs_y, 
        epochs=n_epochs, 
        verbose=2, 
        callbacks=[checkpoint], 
        validation_data=([val_X_img, val_X_txt], val_y)
    )


def train_with_generator(n_epochs=20):
    # Use this if your machine does not have enough memory (as was in my case)

    # Get data information (trs_X_img, trs_X_txt, and trs_y are generated by a generator in the for loop)
    trs_features, trs_descriptions, tokenizer, max_length, vocab_size = data_loading.get_ds_info(C.trs_names_file_path, verbose=True)
    
    # Create model
    model = get_head_model(vocab_size, max_length)

    # Train
    steps = len(trs_descriptions)
    for i in tqdm(range(n_epochs)):
        # Use generator for progressive loading
        generator = data_loading.load_data_generator(trs_features, trs_descriptions, tokenizer, max_length, vocab_size)
        model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)  # fit for one epoch
        filepath = os.path.join(C.MODELS_FOLDER, f'captioning_model-ep{i}.h5')
        model.save(filepath)  # Currently saving at each epoch


if __name__ == "__main__":
    # train_with_full_ds_loaded()
    train_with_generator()
